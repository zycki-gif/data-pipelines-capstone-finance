{"commands": [{"bindings": {}, "collapsed": false, "command": "\n%python\n# ****************************************************************************\n# Utility method to count & print the number of records in each partition.\n# ****************************************************************************\n\ndef printRecordsPerPartition(df):\n  def countInPartition(iterator): yield __builtin__.sum(1 for _ in iterator)\n  results = (df.rdd                   # Convert to an RDD\n    .mapPartitions(countInPartition)  # For each partition, count\n    .collect()                        # Return the counts to the driver\n  )\n  \n  print(\"Per-Partition Counts\")\n  i = 0\n  for result in results: \n    i = i + 1\n    print(\"#{}: {:,}\".format(i, result))\n  \n# ****************************************************************************\n# Utility to count the number of files in and size of a directory\n# ****************************************************************************\n\ndef computeFileStats(path):\n  bytes = 0\n  count = 0\n\n  files = dbutils.fs.ls(path)\n  \n  while (len(files) > 0):\n    fileInfo = files.pop(0)\n    if (fileInfo.isDir() == False):               # isDir() is a method on the fileInfo object\n      count += 1\n      bytes += fileInfo.size                      # size is a parameter on the fileInfo object\n    else:\n      files.extend(dbutils.fs.ls(fileInfo.path))  # append multiple object to files\n      \n  return (count, bytes)\n\n# ****************************************************************************\n# Utility method to cache a table with a specific name\n# ****************************************************************************\n\ndef cacheAs(df, name, level = \"MEMORY-ONLY\"):\n  from pyspark.sql.utils import AnalysisException\n  if level != \"MEMORY-ONLY\":\n    print(\"WARNING: The PySpark API currently does not allow specification of the storage level - using MEMORY-ONLY\")  \n    \n  try: spark.catalog.uncacheTable(name)\n  except AnalysisException: None\n  \n  df.createOrReplaceTempView(name)\n  spark.catalog.cacheTable(name)\n  \n  return df\n\n\n# ****************************************************************************\n# Simplified benchmark of count()\n# ****************************************************************************\n\ndef benchmarkCount(func):\n  import time\n  start = float(time.time() * 1000)                    # Start the clock\n  df = func()\n  total = df.count()                                   # Count the records\n  duration = float(time.time() * 1000) - start         # Stop the clock\n  return (df, total, duration)\n\n# ****************************************************************************\n# Utility methods to terminate streams\n# ****************************************************************************\n\ndef getActiveStreams():\n  try:\n    return spark.streams.active\n  except:\n    # In extream cases, this funtion may throw an ignorable error.\n    print(\"Unable to iterate over all active streams - using an empty set instead.\")\n    return []\n\ndef stopStream(s):\n  try:\n    print(\"Stopping the stream {}.\".format(s.name))\n    s.stop()\n    print(\"The stream {} was stopped.\".format(s.name))\n  except:\n    # In extream cases, this funtion may throw an ignorable error.\n    print(\"An [ignorable] error has occured while stoping the stream.\")\n\ndef stopAllStreams():\n  streams = getActiveStreams()\n  while len(streams) > 0:\n    stopStream(streams[0])\n    streams = getActiveStreams()\n    \n# ****************************************************************************\n# Utility method to wait until the stream is read\n# ****************************************************************************\n\ndef untilStreamIsReady(name, progressions=3):\n  import time\n  queries = list(filter(lambda query: query.name == name, getActiveStreams()))\n\n  while (len(queries) == 0 or len(queries[0].recentProgress) < progressions):\n    time.sleep(5) # Give it a couple of seconds\n    queries = list(filter(lambda query: query.name == name, getActiveStreams()))\n\n  print(\"The stream {} is active and ready.\".format(name))\n\nNone", "commandTitle": "", "commandType": "auto", "commandVersion": 0, "commentThread": [], "commentsVisible": false, "customPlotOptions": {}, "datasetPreviewNameToCmdIdMap": {}, "diffDeletes": [], "diffInserts": [], "displayType": "table", "error": null, "errorSummary": null, "finishTime": 0, "globalVars": {}, "guid": "9d0ebc80-af38-43ea-aa91-b7138d70bc5f", "height": "auto", "hideCommandCode": false, "hideCommandResult": false, "iPythonMetadata": null, "inputWidgets": {}, "latestUser": "", "latestUserId": null, "nuid": "47278fb0-a960-47b1-85b2-2a4698abc080", "origId": 0, "parentHierarchy": [], "pivotAggregation": null, "pivotColumns": null, "position": 1, "results": null, "showCommandTitle": false, "startTime": 0, "state": "finished", "streamStates": {}, "submitTime": 0, "subtype": "command", "version": "CommandV1", "width": "auto", "workflows": [], "xColumns": null, "yColumns": null}, {"bindings": {}, "collapsed": false, "command": "%scala\n\n// ****************************************************************************\n// Utility method to count & print the number of records in each partition.\n// ****************************************************************************\n\ndef printRecordsPerPartition(df:org.apache.spark.sql.Dataset[Row]):Unit = {\n  // import org.apache.spark.sql.functions._\n  val results = df.rdd                                   // Convert to an RDD\n    .mapPartitions(it => Array(it.size).iterator, true)  // For each partition, count\n    .collect()                                           // Return the counts to the driver\n\n  println(\"Per-Partition Counts\")\n  var i = 0\n  for (r <- results) {\n    i = i +1\n    println(\"#%s: %,d\".format(i,r))\n  }\n}\n\n// ****************************************************************************\n// Utility to count the number of files in and size of a directory\n// ****************************************************************************\n\ndef computeFileStats(path:String):(Long,Long) = {\n  var bytes = 0L\n  var count = 0L\n\n  import scala.collection.mutable.ArrayBuffer\n  var files=ArrayBuffer(dbutils.fs.ls(path):_ *)\n\n  while (files.isEmpty == false) {\n    val fileInfo = files.remove(0)\n    if (fileInfo.isDir == false) {\n      count += 1\n      bytes += fileInfo.size\n    } else {\n      files.append(dbutils.fs.ls(fileInfo.path):_ *)\n    }\n  }\n  (count, bytes)\n}\n\n// ****************************************************************************\n// Utility method to cache a table with a specific name\n// ****************************************************************************\n\ndef cacheAs(df:org.apache.spark.sql.DataFrame, name:String, level:org.apache.spark.storage.StorageLevel):org.apache.spark.sql.DataFrame = {\n  try spark.catalog.uncacheTable(name)\n  catch { case _: org.apache.spark.sql.AnalysisException => () }\n  \n  df.createOrReplaceTempView(name)\n  spark.catalog.cacheTable(name, level)\n  return df\n}\n\n// ****************************************************************************\n// Simplified benchmark of count()\n// ****************************************************************************\n\ndef benchmarkCount(func:() => org.apache.spark.sql.DataFrame):(org.apache.spark.sql.DataFrame, Long, Long) = {\n  val start = System.currentTimeMillis            // Start the clock\n  val df = func()                                 // Get our lambda\n  val total = df.count()                          // Count the records\n  val duration = System.currentTimeMillis - start // Stop the clock\n  (df, total, duration)\n}\n\n// ****************************************************************************\n// Benchmarking and cache tracking tool\n// ****************************************************************************\n\ncase class JobResults[T](runtime:Long, duration:Long, cacheSize:Long, maxCacheBefore:Long, remCacheBefore:Long, maxCacheAfter:Long, remCacheAfter:Long, result:T) {\n  def printTime():Unit = {\n    if (runtime < 1000)                 println(f\"Runtime:  ${runtime}%,d ms\")\n    else if (runtime < 60 * 1000)       println(f\"Runtime:  ${runtime/1000.0}%,.2f sec\")\n    else if (runtime < 60 * 60 * 1000)  println(f\"Runtime:  ${runtime/1000.0/60.0}%,.2f min\")\n    else                                println(f\"Runtime:  ${runtime/1000.0/60.0/60.0}%,.2f hr\")\n    \n    if (duration < 1000)                println(f\"All Jobs: ${duration}%,d ms\")\n    else if (duration < 60 * 1000)      println(f\"All Jobs: ${duration/1000.0}%,.2f sec\")\n    else if (duration < 60 * 60 * 1000) println(f\"All Jobs: ${duration/1000.0/60.0}%,.2f min\")\n    else                                println(f\"Job Dur: ${duration/1000.0/60.0/60.0}%,.2f hr\")\n  }\n  def printCache():Unit = {\n    if (Math.abs(cacheSize) < 1024)                    println(f\"Cached:   ${cacheSize}%,d bytes\")\n    else if (Math.abs(cacheSize) < 1024 * 1024)        println(f\"Cached:   ${cacheSize/1024.0}%,.3f KB\")\n    else if (Math.abs(cacheSize) < 1024 * 1024 * 1024) println(f\"Cached:   ${cacheSize/1024.0/1024.0}%,.3f MB\")\n    else                                               println(f\"Cached:   ${cacheSize/1024.0/1024.0/1024.0}%,.3f GB\")\n    \n    println(f\"Before:   ${remCacheBefore / 1024.0 / 1024.0}%,.3f / ${maxCacheBefore / 1024.0 / 1024.0}%,.3f MB / ${100.0*remCacheBefore/maxCacheBefore}%.2f%%\")\n    println(f\"After:    ${remCacheAfter / 1024.0 / 1024.0}%,.3f / ${maxCacheAfter / 1024.0 / 1024.0}%,.3f MB / ${100.0*remCacheAfter/maxCacheAfter}%.2f%%\")\n  }\n  def print():Unit = {\n    printTime()\n    printCache()\n  }\n}\n\ncase class Node(driver:Boolean, executor:Boolean, address:String, maximum:Long, available:Long) {\n  def this(address:String, maximum:Long, available:Long) = this(address.contains(\"-\"), !address.contains(\"-\"), address, maximum, available)\n}\n\nclass Tracker() extends org.apache.spark.scheduler.SparkListener() {\n  \n  sc.addSparkListener(this)\n  \n  val jobStarts = scala.collection.mutable.Map[Int,Long]()\n  val jobEnds = scala.collection.mutable.Map[Int,Long]()\n  \n  def track[T](func:() => T):JobResults[T] = {\n    jobEnds.clear()\n    jobStarts.clear()\n\n    val executorsBefore = sc.getExecutorMemoryStatus.map(x => new Node(x._1, x._2._1, x._2._2)).filter(_.executor)\n    val maxCacheBefore = executorsBefore.map(_.maximum).sum\n    val remCacheBefore = executorsBefore.map(_.available).sum\n    \n    val start = System.currentTimeMillis()\n    val result = func()\n    val runtime = System.currentTimeMillis() - start\n    \n    Thread.sleep(1000) // give it a second to catch up\n\n    val executorsAfter = sc.getExecutorMemoryStatus.map(x => new Node(x._1, x._2._1, x._2._2)).filter(_.executor)\n    val maxCacheAfter = executorsAfter.map(_.maximum).sum\n    val remCacheAfter = executorsAfter.map(_.available).sum\n\n    var duration = 0L\n    \n    for ((jobId, startAt) <- jobStarts) {\n      assert(jobEnds.keySet.exists(_ == jobId), s\"A conclusion for Job ID $jobId was not found.\") \n      duration += jobEnds(jobId) - startAt\n    }\n    JobResults(runtime, duration, remCacheBefore-remCacheAfter, maxCacheBefore, remCacheBefore, maxCacheAfter, remCacheAfter, result)\n  }\n  override def onJobStart(jobStart: org.apache.spark.scheduler.SparkListenerJobStart):Unit = jobStarts.put(jobStart.jobId, jobStart.time)\n  override def onJobEnd(jobEnd: org.apache.spark.scheduler.SparkListenerJobEnd): Unit = jobEnds.put(jobEnd.jobId, jobEnd.time)\n}\n\nval tracker = new Tracker()\n\n// ****************************************************************************\n// Utility methods to terminate streams\n// ****************************************************************************\n\ndef getActiveStreams():Seq[org.apache.spark.sql.streaming.StreamingQuery] = {\n  return try {\n    spark.streams.active\n  } catch {\n    case e:Throwable => {\n      // In extream cases, this funtion may throw an ignorable error.\n      println(\"Unable to iterate over all active streams - using an empty set instead.\")\n      Seq[org.apache.spark.sql.streaming.StreamingQuery]()\n    }\n  }\n}\n\ndef stopStream(s:org.apache.spark.sql.streaming.StreamingQuery):Unit = {\n  try {\n    s.stop()\n  } catch {\n    case e:Throwable => {\n      // In extream cases, this funtion may throw an ignorable error.\n      println(s\"An [ignorable] error has occured while stoping the stream.\")\n    }\n  }\n}\n\ndef stopAllStreams():Unit = {\n  var streams = getActiveStreams()\n  while (streams.length > 0) {\n    stopStream(streams(0))\n    streams = getActiveStreams()\n  }\n}\n\n// ****************************************************************************\n// Utility method to wait until the stream is read\n// ****************************************************************************\n\ndef untilStreamIsReady(name:String, progressions:Int = 3):Unit = {\n  var queries = getActiveStreams().filter(_.name == name)\n  \n  while (queries.length == 0 || queries(0).recentProgress.length < progressions) {\n    Thread.sleep(5*1000) // Give it a couple of seconds\n    queries = getActiveStreams().filter(_.name == name)\n  }\n  println(\"The stream %s is active and ready.\".format(name))\n}\n\ndisplayHTML(\"Defining user-facing utility methods...\")\n", "commandTitle": "", "commandType": "auto", "commandVersion": 0, "commentThread": [], "commentsVisible": false, "customPlotOptions": {}, "datasetPreviewNameToCmdIdMap": {}, "diffDeletes": [], "diffInserts": [], "displayType": "table", "error": null, "errorSummary": null, "finishTime": 0, "globalVars": {}, "guid": "286eaa03-f786-4932-be96-6061187a5647", "height": "auto", "hideCommandCode": false, "hideCommandResult": false, "iPythonMetadata": null, "inputWidgets": {}, "latestUser": "", "latestUserId": null, "nuid": "664a741b-b8ac-45e5-be61-aa5231fe2151", "origId": 0, "parentHierarchy": [], "pivotAggregation": null, "pivotColumns": null, "position": 2, "results": null, "showCommandTitle": false, "startTime": 0, "state": "finished", "streamStates": {}, "submitTime": 0, "subtype": "command", "version": "CommandV1", "width": "auto", "workflows": [], "xColumns": null, "yColumns": null}], "dashboards": [], "globalVars": {}, "guid": "9c9795fa-7278-4f1c-a430-ce49b151ec3f", "iPythonMetadata": null, "inputWidgets": {}, "language": "python", "name": "Utility-Methods", "origId": 0, "version": "NotebookV1"}